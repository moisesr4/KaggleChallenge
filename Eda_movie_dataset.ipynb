{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('ggplot')\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, KFold\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import StandardScaler\nstop = set(stopwords.words('english'))\nimport os\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nimport json\nimport ast\nimport eli5\nimport shap\nfrom catboost import CatBoostRegressor\nfrom urllib.request import urlopen\nfrom PIL import Image\nfrom sklearn.preprocessing import LabelEncoder\nimport time\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import linear_model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \n        \n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tmdb-box-office-prediction/train.csv')\ntest = pd.read_csv('../input/tmdb-box-office-prediction/test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Delete the redundant column which does not add any inormation\ntrain.drop(columns=['homepage', 'imdb_id', 'original_title', 'poster_path'],axis = 1 ,inplace=True)\n\ntest.drop(columns=['homepage', 'imdb_id', 'original_title', 'poster_path'],axis = 1 ,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of null values in the data\n\ntrain.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to plot the null values in the different columns\nnull_values = train.isna().sum().reset_index()\n\n\nx = null_values.iloc[:,0]\ny = null_values.iloc[:,1]\n\nx = x.tolist()\ny = y.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(16, 8))\nplt.xticks(rotation='vertical')\nplt.bar(x, y ,label='Null values count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/gravix/gradient-in-a-box\n\ndict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\ndef text_to_dict(df):\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n    return df\n        \ntrain = text_to_dict(train)\ntest = text_to_dict(test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape of train and test data set\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To find if the film belongs to any big collection\n# to do\n# normalize\n\n\ntrain['has_collection'] = train['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)\n\ntest['has_collection'] = test['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)\n\ntrain = train.drop(['belongs_to_collection'], axis=1)\ntest = test.drop(['belongs_to_collection'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data processing for the Genre columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from typing import Iterable \nfrom collections import Iterable\n\n\ndef flatten(items):\n    \"\"\"Yield items from any nested iterable; see Reference.\"\"\"\n    for x in items:\n        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n            for sub_x in flatten(x):\n                yield sub_x\n        else:\n            yield x\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"            \nTop_genre = Counter(list(flatten(list(train['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)))).most_common(10)\n\nAll_genre = Counter(list(flatten(list(train['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values))))\n\n# to convert the tuple into the dict\ndict_top_genre = dict(All_genre)\n\nTotal_genres_count = sum(Counter(list(flatten(list(train['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)))).values())\n\nprint(Top_genre)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['num_genres'] = train['genres'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_genres'] = train['genres'].apply(lambda x: ','.join(sorted([i['name'] for i in x])) if x != {} else '')\n\n# max and sum\ntrain['max_genres_norm'] = train['all_genres'].apply(lambda x: max([dict_top_genre[i]/Total_genres_count for i in x.split(',')]) if x != '' else 0)\n\n## create a column based on top genre movies\n\ntest['num_genres'] = test['genres'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_genres'] = test['genres'].apply(lambda x: ','.join(sorted([i['name'] for i in x])) if x != {} else '')\n\n# max and sum\ntest['max_genres_norm'] = test['all_genres'].apply(lambda x: max([dict_top_genre[i]/Total_genres_count for i in x.split(',')]) if x != '' else 0)\n\n\ntrain = train.drop(['genres','all_genres'], axis=1)\ntest = test.drop(['genres','all_genres'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Production Companies\n\n#### production compnaies plays a important role for the movie.. often top rated production companies tend to earn good revenue. so parsing the column product companies to get more details from it."},{"metadata":{"trusted":true},"cell_type":"code","source":"Top_production_companies = Counter(list(flatten(list(train['production_companies'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)))).most_common(30)\nprint(Top_production_companies)\n\nAll_production_companies = Counter(list(flatten(list(train['production_companies'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values))))\n\n# to convert the tuple into the dict\ndict_top_production_companies = dict(Top_production_companies)\n\nTop_production_companies_count = sum(dict_top_production_companies.values())\nprint('Total count of the production companies',Top_production_companies_count)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['num_companies'] = train['production_companies'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_production_companies'] = train['production_companies'].apply(lambda x: ','.join(sorted([i['name'] for i in x])) if x != {} else '')\n\ntrain['max_production_companies_norm'] = train['all_production_companies'].apply(lambda x: max([dict_top_production_companies[i]/Top_production_companies_count if i in dict_top_production_companies.keys() else 0 for i in x.split(',')]) if x != '' else 0)\n\n\n# for production in Top_production_companies:\n#     train['production_company_' + production[0]] = train['all_production_companies'].apply(lambda x: 1 if production[0] in x else 0)\n\ntest['num_companies'] = test['production_companies'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_production_companies'] = test['production_companies'].apply(lambda x: ','.join(sorted([i['name'] for i in x])) if x != {} else '')\n\ntest['max_production_companies_norm'] = test['all_production_companies'].apply(lambda x: max([dict_top_production_companies[i]/Top_production_companies_count if i in dict_top_production_companies.keys() else 0 for i in x.split(',')]) if x != '' else 0)\n\n\n# for production in Top_production_companies:\n#     test['production_company_' + production[0]] = test['all_production_companies'].apply(lambda x: 1 if production[0] in x else 0)\n\n\n\ntrain = train.drop(['production_companies', 'all_production_companies'], axis=1)\ntest = test.drop(['production_companies', 'all_production_companies'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Production Countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_countries = list(train['production_countries'].apply(lambda x: [i['iso_3166_1'] for i in x] if x != {} else []).values)\nTop_countries = Counter([i for j in list_of_countries for i in j]).most_common(30)\n\n\n# to convert the tuple into the dict\ndict_top_production_countries = dict(Top_countries)\n\nTop_production_countries_count = sum(dict_top_production_countries.values())\nprint('Total movie count of the production country',Top_production_countries_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['num_production_countries'] = train['production_countries'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_production_countries'] = train['production_countries'].apply(lambda x: ','.join(sorted([i['iso_3166_1'] for i in x])) if x != {} else '')\n\n\ntrain['max_production_country_norm'] = train['all_production_countries'].apply(lambda x: max([dict_top_production_countries[i]/Top_production_countries_count if i in dict_top_production_countries.keys() else 0 for i in x.split(',')]) if x != '' else 0)\n\n# for country in Top_countries:\n#     train['production_country_' + country[0]] = train['all_countries'].apply(lambda x: 1 if country[0] in x else 0)\n\ntest['num_production_countries'] = test['production_countries'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_production_countries'] = test['production_countries'].apply(lambda x: ','.join(sorted([i['iso_3166_1'] for i in x])) if x != {} else '')\n\n\ntest['max_production_country_norm'] = test['all_production_countries'].apply(lambda x: max([dict_top_production_countries[i]/Top_production_countries_count if i in dict_top_production_countries.keys() else 0 for i in x.split(',')]) if x != '' else 0)\n\n\n\n# # for country in Top_countries:\n#     test['production_country_' + country[0]] = test['all_countries'].apply(lambda x: 1 if country[0] in x else 0)\n    \ntrain = train.drop(['num_production_countries', 'all_production_countries'], axis=1)\ntest = test.drop(['num_production_countries', 'all_production_countries'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Spoken language"},{"metadata":{"trusted":true},"cell_type":"code","source":"## feature engineering part has to take care production country and spoken language","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_languages = list(train['spoken_languages'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ntop_language = Counter([i for j in list_of_languages for i in j]).most_common(5)\n\ndict_top_language = dict(top_language)\n\nTop_languages_count = sum(dict_top_language.values())\nprint('Total movie count of the production country',Top_languages_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['num_languages'] = train['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_languages'] = train['spoken_languages'].apply(lambda x: ','.join(sorted([i['name'] for i in x])) if x != {} else '')\n\ntrain['max_language_norm'] = train['all_languages'].apply(lambda x: max([dict_top_language[i]/Top_languages_count if i in dict_top_language.keys() else 0 for i in x.split(',')]) if x != '' else 0)\n\n\n# train['max_language_norm'] = train['all_languages'].apply(lambda x: max([dict_top_language[i]/Top_languages_count for i in x.split(',')]) if x != '' else 0)\n\n# top_languages = [m[0] for m in Counter([i for j in list_of_languages for i in j]).most_common(30)]\n# for g in top_languages:\n#     train['language_' + g] = train['all_languages'].apply(lambda x: 1 if g in x else 0)\n    \n# test['num_languages'] = test['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)\n# test['all_languages'] = test['spoken_languages'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\n# for g in top_languages:\n#     test['language_' + g] = test['all_languages'].apply(lambda x: 1 if g in x else 0)\n\n# train = train.drop(['spoken_languages', 'all_languages'], axis=1)\n# test = test.drop(['spoken_languages', 'all_languages'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cast_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_cast_names = list(train['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ntop_cast_names = Counter([i for j in list_of_cast_names for i in j]).most_common(500)\n\ndict_cast_names = dict(top_cast_names)\n\nTop_cast_names_count = sum(dict_cast_names.values())\nprint('Total movie count acted wrt cast names',Top_cast_names_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_cast_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_cast'] = train['cast'].apply(lambda x: ','.join(sorted([i['name'] for i in x])) if x != {} else '')\n\ntrain['max_cast_norm'] = train['all_cast'].apply(lambda x: max([dict_cast_names[i]/Top_cast_names_count if i in dict_cast_names.keys() else 0 for i in x.split(',')]) if x != '' else 0)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cast Gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_cast_genders = list(train['cast'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_cast_genders for i in j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\n\ntrain['genders_0_cast'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntrain['genders_1_cast'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntrain['genders_2_cast'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n\n\ntest['num_cast'] = test['cast'].apply(lambda x: len(x) if x != {} else 0)\n\ntest['genders_0_cast'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntest['genders_1_cast'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntest['genders_2_cast'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n\n# train = train.drop(['cast'], axis=1)\n# test = test.drop(['cast'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_crew_departments = list(train['crew'].apply(lambda x: [i['job'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_crew_departments for i in j]).most_common(14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top director \n\nlist_directors_name = list(train['crew'].apply(lambda x: [i['name'] for i in x if i['job'] == 'Director' ] if x != {} else []).values)\ndict_director_names = dict(Counter([i for j in list_directors_name for i in j]))\n\nTop_director_names_count = sum(dict_director_names.values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top producer\n\n\nlist_producer_name = list(train['crew'].apply(lambda x: [i['name'] for i in x if i['job'] == 'Producer' ] if x != {} else []).values)\ndict_producer_names = dict(Counter([i for j in list_producer_name for i in j]))\n\nTop_producer_names_count = sum(dict_producer_names.values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top Original Music Composer\n\n\nlist_music_composer_name = list(train['crew'].apply(lambda x: [i['name'] for i in x if i['job'] == 'Original Music Composer' ] if x != {} else []).values)\ndict_composer_names = dict(Counter([i for j in list_music_composer_name for i in j]).most_common(15))\n\nTop_composer_names_count = sum(dict_composer_names.values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['num_crew'] = train['crew'].apply(lambda x: len(x) if x != {} else 0)\n\ntrain['crew_director'] = train['crew'].apply(lambda x: ','.join(sorted([i['name'] for i in x if i['job'] == 'Director'])) if x != {} else '')\ntrain['crew_director_norm'] = train['crew_director'].apply(lambda x: [(dict_director_names[i]/Top_director_names_count) if i in dict_director_names.keys() else 0 for i in x.split(',')][0] if x != '' else 0)\ntrain['crew_director_log_norm'] = train['crew_director'].apply(lambda x: [np.log(dict_director_names[i])+1 if i in dict_director_names.keys() else 1 for i in x.split(',')][0] if x != '' else 0)\n\ntrain['crew_producer'] = train['crew'].apply(lambda x: ','.join(sorted([i['name'] for i in x if i['job'] == 'Producer'])) if x != {} else '')\ntrain['crew_producer_norm'] = train['crew_producer'].apply(lambda x: [(dict_producer_names[i]/Top_producer_names_count) if i in dict_producer_names.keys() else 0 for i in x.split(',')][0] if x != '' else 0)\ntrain['crew_producer_log_norm'] = train['crew_producer'].apply(lambda x: [np.log(dict_producer_names[i])+1 if i in dict_producer_names.keys() else 1 for i in x.split(',')][0] if x != '' else 0)\n\ntrain['crew_composer'] = train['crew'].apply(lambda x: ','.join(sorted([i['name'] for i in x if i['job'] == 'Original Music Composer'])) if x != {} else '')\ntrain['crew_composer_norm'] = train['crew_composer'].apply(lambda x: [(dict_composer_names[i]/Top_composer_names_count) if i in dict_composer_names.keys() else 0 for i in x.split(',')][0] if x != '' else 0)\ntrain['crew_composer_log_norm'] = train['crew_composer'].apply(lambda x: [np.log(dict_composer_names[i])+1 if i in dict_composer_names.keys() else 1 for i in x.split(',')][0] if x != '' else 0)\n\n\n\n\n#############################################################################################################    \n\n# test['num_crew'] = test['crew'].apply(lambda x: len(x) if x != {} else 0)\n\n\n\n\n# train = train.drop(['crew'], axis=1)\n# test = test.drop(['crew'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import date\nimport calendar\n\ntrain.release_date = pd.to_datetime(train.release_date)\ntrain.loc[:,\"Year\"] = train[\"release_date\"].dt.year\ntrain.loc[:,\"Month\"] = train[\"release_date\"].dt.month\ntrain['day_of_week'] = train['release_date'].apply(lambda x: calendar.day_name[x.weekday()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.get_dummies(train, columns=[\"day_of_week\",\"Month\"]).head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (16, 6))\nplt.subplot(1, 2, 1)\nplt.hist(train['revenue']);\nplt.title('Distribution of revenue');\nplt.subplot(1, 2, 2)\nplt.hist(np.log1p(train['revenue']));\nplt.title('Distribution of log of revenue');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['log_revenue'] = np.log1p(train['revenue'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (16, 6))\nplt.subplot(1, 2, 1)\nplt.hist(train['budget']);\nplt.title('Distribution of budget');\nplt.subplot(1, 2, 2)\nplt.hist(np.log1p(train['budget']));\nplt.title('Distribution of log of budget');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['log_budget'] = np.log1p(train['budget'])\ntest['log_budget'] = np.log1p(test['budget'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['homepage'].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['has_homepage'] = 0\ntrain.loc[train['homepage'].isnull() == False, 'has_homepage'] = 1\ntest['has_homepage'] = 0\ntest.loc[test['homepage'].isnull() == False, 'has_homepage'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='has_homepage', y='revenue', data=train);\nplt.title('Revenue for film with and without homepage');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nsns.boxplot(x='original_language', y='revenue', data=train.loc[train['original_language'].isin(train['original_language'].value_counts().head(10).index)]);\nplt.title('Mean revenue per language');\nplt.subplot(1, 2, 2)\nsns.boxplot(x='original_language', y='log_revenue', data=train.loc[train['original_language'].isin(train['original_language'].value_counts().head(10).index)]);\nplt.title('Mean log revenue per language');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 12))\ntext = ' '.join(train['original_title'].values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top words in titles')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 12))\ntext = ' '.join(train['overview'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top words in overview')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['overview']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(sublinear_tf=True,\n            analyzer='word',\n            token_pattern=r'\\w{1,}',\n            ngram_range=(1, 2),\n            min_df=5)\n\noverview_text = vectorizer.fit_transform(train['overview'].fillna(''))\nlinreg = LinearRegression()\nlinreg.fit(overview_text, train['log_revenue'])\neli5.show_weights(linreg, vec=vectorizer, top=20, feature_filter=lambda x: x != '<BIAS>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(\n            sublinear_tf=True,\n            analyzer='word',\n            token_pattern=r'\\w{1,}',\n            ngram_range=(1, 2),\n            min_df=5)\n\noverview_text = vectorizer.fit_transform(train['overview'].fillna(''))\nlinreg = LinearRegression()\nlinreg.fit(overview_text, train['log_revenue'])\neli5.show_weights(linreg, vec=vectorizer, top=20, feature_filter=lambda x: x != '<BIAS>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Target value:', train['log_revenue'][1000])\neli5.show_prediction(linreg, doc=train['overview'].values[1000], vec=vectorizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(train['popularity'], train['revenue'])\nplt.title('Revenue vs popularity');\nplt.subplot(1, 2, 2)\nplt.scatter(train['popularity'], train['log_revenue'])\nplt.title('Log Revenue vs popularity');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.loc[test['release_date'].isnull() == True, 'release_date'] = '01/01/98'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fix_date(x):\n    \"\"\"\n    Fixes dates which are in 20xx\n    \"\"\"\n    year = x.split('/')[2]\n    if int(year) <= 19:\n        return x[:-2] + '20' + year\n    else:\n        return x[:-2] + '19' + year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['release_date'] = train['release_date'].apply(lambda x: fix_date(x))\ntest['release_date'] = test['release_date'].apply(lambda x: fix_date(x))\ntrain['release_date'] = pd.to_datetime(train['release_date'])\ntest['release_date'] = pd.to_datetime(test['release_date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating features based on dates\ndef process_date(df):\n    date_parts = [\"year\", \"weekday\", \"month\", 'weekofyear', 'day', 'quarter']\n    for part in date_parts:\n        part_col = 'release_date' + \"_\" + part\n        df[part_col] = getattr(df['release_date'].dt, part).astype(int)\n    \n    return df\n\ntrain = process_date(train)\ntest = process_date(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d1 = train['release_date_year'].value_counts().sort_index()\nd2 = test['release_date_year'].value_counts().sort_index()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='train'), go.Scatter(x=d2.index, y=d2.values, name='test')]\nlayout = go.Layout(dict(title = \"Number of films per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d1 = train['release_date_year'].value_counts().sort_index()\nd2 = train.groupby(['release_date_year'])['revenue'].sum()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='film count'), go.Scatter(x=d2.index, y=d2.values, name='total revenue', yaxis='y2')]\nlayout = go.Layout(dict(title = \"Number of films and total revenue per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  yaxis2=dict(title='Total revenue', overlaying='y', side='right')\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d1 = train['release_date_year'].value_counts().sort_index()\nd2 = train.groupby(['release_date_year'])['revenue'].mean()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='film count'), go.Scatter(x=d2.index, y=d2.values, name='mean revenue', yaxis='y2')]\nlayout = go.Layout(dict(title = \"Number of films and average revenue per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  yaxis2=dict(title='Average revenue', overlaying='y', side='right')\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='release_date_weekday', y='revenue', data=train);\nplt.title('Revenue on different days of week of release');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 6))\nplt.subplot(1, 3, 1)\nplt.hist(train['runtime'].fillna(0) / 60, bins=40);\nplt.title('Distribution of length of film in hours');\nplt.subplot(1, 3, 2)\nplt.scatter(train['runtime'].fillna(0), train['revenue'])\nplt.title('runtime vs revenue');\nplt.subplot(1, 3, 3)\nplt.scatter(train['runtime'].fillna(0), train['popularity'])\nplt.title('runtime vs popularity');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['status'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['status'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 12))\ntext = ' '.join(train['tagline'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top words in tagline')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='has_collection', y='revenue', data=train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='num_genres', y='revenue', data=train);\nplt.title('Revenue for different number of genres in the film');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(x='genre_Drama', y='revenue', data=train[:100]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(3, 5, figsize=(24, 12))\nplt.suptitle('Violinplot of revenue vs genres')\nfor i, e in enumerate([col for col in train.columns if 'genre_' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(6, 5, figsize=(24, 32))\nplt.suptitle('Violinplot of revenue vs production company')\nfor i, e in enumerate([col for col in train.columns if 'production_company' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='num_countries', y='revenue', data=train);\nplt.title('Revenue for different number of countries producing the film');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(5, 5, figsize=(24, 32))\nplt.suptitle('Violinplot of revenue vs production country')\nfor i, e in enumerate([col for col in train.columns if 'production_country' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(train['num_cast'], train['revenue'])\nplt.title('Number of cast members vs revenue');\nplt.subplot(1, 2, 2)\nplt.scatter(train['num_cast'], train['log_revenue'])\nplt.title('Log Revenue vs number of cast members');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(3, 5, figsize=(24, 18))\nplt.suptitle('Violinplot of revenue vs cast')\nfor i, e in enumerate([col for col in train.columns if 'cast_name' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(3, 5, figsize=(24, 18))\nplt.suptitle('Violinplot of revenue vs cast')\nfor i, e in enumerate([col for col in train.columns if 'cast_character_' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(6, 5, figsize=(24, 32))\nplt.suptitle('Violinplot of revenue vs keyword')\nfor i, e in enumerate([col for col in train.columns if 'keyword_' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(train['num_crew'], train['revenue'])\nplt.title('Number of crew members vs revenue');\nplt.subplot(1, 2, 2)\nplt.scatter(train['num_crew'], train['log_revenue'])\nplt.title('Log Revenue vs number of crew members');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(3, 5, figsize=(24, 18))\nplt.suptitle('Violinplot of revenue vs crew_character')\nfor i, e in enumerate([col for col in train.columns if 'crew_character_' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(3, 5, figsize=(24, 18))\nplt.suptitle('Violinplot of revenue vs jobs')\nfor i, e in enumerate([col for col in train.columns if 'jobs_' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['homepage', 'imdb_id', 'poster_path', 'release_date', 'status', 'log_revenue'], axis=1)\ntest = test.drop(['homepage', 'imdb_id', 'poster_path', 'release_date', 'status'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train.columns:\n    if train[col].nunique() == 1:\n        print(col)\n        train = train.drop([col], axis=1)\n        test = test.drop([col], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['original_language', 'collection_name']:\n    le = LabelEncoder()\n    le.fit(list(train[col].fillna('')) + list(test[col].fillna('')))\n    train[col] = le.transform(train[col].fillna('').astype(str))\n    test[col] = le.transform(test[col].fillna('').astype(str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_texts = train[['title', 'tagline', 'overview', 'original_title']]\ntest_texts = test[['title', 'tagline', 'overview', 'original_title']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['title', 'tagline', 'overview', 'original_title']:\n    train['len_' + col] = train[col].fillna('').apply(lambda x: len(str(x)))\n    train['words_' + col] = train[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n    train = train.drop(col, axis=1)\n    test['len_' + col] = test[col].fillna('').apply(lambda x: len(str(x)))\n    test['words_' + col] = test[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n    test = test.drop(col, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['id', 'revenue'], axis=1)\ny = np.log1p(train['revenue'])\nX_test = test.drop(['id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 30,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 5,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\nmodel1 = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\nmodel1.fit(X_train, y_train, \n        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n        verbose=1000, early_stopping_rounds=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(model1, feature_filter=lambda x: x != '<BIAS>')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}